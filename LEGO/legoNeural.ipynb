{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lego_Sets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.list_price = df.list_price.astype(float)\n",
    "df['review_difficulty'] = df['review_difficulty'].astype(\"category\")\n",
    "df['review_difficulty'] = df['review_difficulty'].cat.reorder_categories(['Very Easy',\n",
    "                                                               'Easy',\n",
    "                                                               'Average',\n",
    "                                                               'Challenging',\n",
    "                                                               'Very Challenging'],\n",
    "                                                                        ordered = True)\n",
    "df['review_difficulty'] = df['review_difficulty'].cat.codes\n",
    "df.theme_name = df.theme_name.astype(\"category\")\n",
    "df.ages = df.ages.astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy().dropna()\n",
    "df1 = df1.drop(['prod_desc', \n",
    "                'prod_id', \n",
    "                'prod_long_desc', \n",
    "                'set_name',\n",
    "                'theme_name',\n",
    "                'country'], \n",
    "                 axis=1)\n",
    "df2 = pd.get_dummies(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df2.drop(['list_price'], axis=1), df2['list_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                                    X, \n",
    "                                                    Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(momentum = 0.0,\n",
    "                activation = 'softplus', #git\n",
    "                learn_rate = 0.1, #git\n",
    "                dropout_rate = 0.0,\n",
    "                weight_constraint = 1,\n",
    "                neurons = 100, #git\n",
    "                init = 'lecun_uniform', #git\n",
    "                optimizer = 'adagrad'): #git\n",
    "\n",
    "    # create model\n",
    "    NN_model = Sequential()\n",
    "    NN_model.add(Dense(128, \n",
    "                   kernel_initializer=init,\n",
    "                   input_dim = X_train.shape[1], \n",
    "                   activation=activation))\n",
    "\n",
    "    # The Hidden Layers :\n",
    "    NN_model.add(Dense(neurons, \n",
    "                       kernel_initializer=init,\n",
    "                       activation=activation,\n",
    "                      kernel_constraint=maxnorm(weight_constraint)))\n",
    "    NN_model.add(Dense(neurons, \n",
    "                       kernel_initializer=init,\n",
    "                       activation=activation,\n",
    "                      kernel_constraint=maxnorm(weight_constraint)))\n",
    "    NN_model.add(Dropout(dropout_rate)) \n",
    "    NN_model.add(Dense(neurons, \n",
    "                       kernel_initializer=init,\n",
    "                       activation=activation,\n",
    "                      kernel_constraint=maxnorm(weight_constraint)))\n",
    "\n",
    "    # The Output Layer :\n",
    "    NN_model.add(Dense(1, kernel_initializer=init,\n",
    "                       activation='linear'))\n",
    "\n",
    "    \n",
    "    NN_model.compile(loss='mean_squared_error', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['mean_absolute_error',\n",
    "                           'mean_squared_error'])\n",
    "    return NN_model\n",
    "\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, \n",
    "                        epochs = 50, \n",
    "                        batch_size = 32,\n",
    "                        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grid_search(model = model):\n",
    "# Use scikit-learn to grid search \n",
    "    activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "    momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    weight_constraint=[1, 2, 3, 4, 5]\n",
    "    neurons = [30, 100, 128, 256]\n",
    "    init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "    optimizer = [ 'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    ##############################################################\n",
    "    # grid search epochs, batch size\n",
    "    epochs = [50, 100] # add 50, 100, 150 etc\n",
    "    batch_size = [10, 32, 64] # add 5, 10, 20, 40, 60, 80, 100 etc\n",
    "    param_grid = dict(# epochs=epochs,\n",
    "    #                  batch_size=batch_size,\n",
    "#                      activation=activation,\n",
    "    #                  momentum=momentum,\n",
    "    #                  learn_rate=learn_rate,\n",
    "                     dropout_rate=dropout_rate,\n",
    "                     weight_constraint=weight_constraint,\n",
    "#                      neurons=neurons,\n",
    "#                      init=init,\n",
    "#                      optimizer=optimizer\n",
    "                     )\n",
    "    ##############################################################\n",
    "    grid = GridSearchCV(estimator=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring=['r2'],\n",
    "                        refit = 'r2',\n",
    "                        n_jobs=1,\n",
    "                        cv = 2)\n",
    "    grid_result = grid.fit(X_train, Y_train) \n",
    "    ##############################################################\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_r2']\n",
    "    stds = grid_result.cv_results_['std_test_r2']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        \n",
    "grid_search(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "testing = create_model()\n",
    "neural_pred = list(testing.predict(X_test)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_forecast = ({'Actual': Y_test, 'Predicted': neural_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 62.588226409567135\n",
      "Mean Squared Error: 9916.084338695577\n",
      "Root Mean Squared Error: 99.57953775096355\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, neural_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, neural_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, neural_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
