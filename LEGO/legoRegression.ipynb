{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lego_Sets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.list_price = df.list_price.astype(float)\n",
    "df['review_difficulty'] = df['review_difficulty'].astype(\"category\")\n",
    "df['review_difficulty'] = df['review_difficulty'].cat.reorder_categories(['Very Easy',\n",
    "                                                               'Easy',\n",
    "                                                               'Average',\n",
    "                                                               'Challenging',\n",
    "                                                               'Very Challenging'],\n",
    "                                                                        ordered = True)\n",
    "df['review_difficulty'] = df['review_difficulty'].cat.codes\n",
    "df.theme_name = df.theme_name.astype(\"category\")\n",
    "df.ages = df.ages.astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy().dropna()\n",
    "df1 = df1.drop(['prod_desc', \n",
    "                'prod_id', \n",
    "                'prod_long_desc', \n",
    "                'set_name',\n",
    "                'theme_name',\n",
    "                'country'], \n",
    "                 axis=1)\n",
    "df2 = pd.get_dummies(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df2.drop(['list_price'], axis=1), df2['list_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                                    X, \n",
    "                                                    Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "\n",
    "\n",
    "# with statsmodels\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)# adding a constant\n",
    " \n",
    "model = sm.OLS(Y_train, X_train).fit()\n",
    "Y_pred = model.predict(X_test) \n",
    " \n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_forecast = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\n",
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.hist(figsize = (12,10))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df1['review_difficulty'], df1['list_price'], color='green')\n",
    "plt.title('List price vs Review difficulty', fontsize=14)\n",
    "plt.xlabel('Review difficulty', fontsize=14)\n",
    "plt.ylabel('List price', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df1['num_reviews'], df1['list_price'], color='green')\n",
    "plt.title('List price vs Number of reviews', fontsize=14)\n",
    "plt.xlabel('Number of reviews', fontsize=14)\n",
    "plt.ylabel('List price', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df1['piece_count'], df1['list_price'], color='green')\n",
    "plt.title('List price vs Piece count', fontsize=14)\n",
    "plt.xlabel('Piece count', fontsize=14)\n",
    "plt.ylabel('List price', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df1['play_star_rating'], df1['list_price'], color='green')\n",
    "plt.title('List price vs Play star rating', fontsize=14)\n",
    "plt.xlabel('Play star rating', fontsize=14)\n",
    "plt.ylabel('List price', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NN_model = Sequential()\n",
    "\n",
    "# # The Input Layer :\n",
    "# NN_model.add(Dense(128, \n",
    "#                    kernel_initializer='normal',\n",
    "#                    input_dim = X_train.shape[1], \n",
    "#                    activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# NN_model.add(Dense(256, \n",
    "#                    kernel_initializer='normal',\n",
    "#                    activation='relu'))\n",
    "# NN_model.add(Dense(256, \n",
    "#                    kernel_initializer='normal',\n",
    "#                    activation='relu'))\n",
    "# NN_model.add(Dense(256, \n",
    "#                    kernel_initializer='normal',\n",
    "#                    activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# NN_model.add(Dense(1, kernel_initializer='normal',\n",
    "#                    activation='linear'))\n",
    "\n",
    "# # Compile the network :\n",
    "# NN_model.compile(loss='mean_squared_error',\n",
    "#                         momentum = 0.0,\n",
    "#                         activation = 'relu',\n",
    "#                         learn_rate = 0.1,\n",
    "#                         dropout_rate = 0.0,\n",
    "#                         weight_constraint = 1,\n",
    "#                         neurons = 1,\n",
    "#                         init = 'uniform',\n",
    "#                         optimizer = 'adam',\n",
    "#                         metrics=['mean_absolute_error',\n",
    "#                                  'mean_squared_error'])\n",
    "# NN_model.summary()\n",
    "\n",
    "def create_model():\n",
    "    momentum = 0.0\n",
    "    activation = 'relu'\n",
    "    learn_rate = 0.1\n",
    "    dropout_rate = 0.0\n",
    "    weight_constraint = 1\n",
    "    neurons = 1\n",
    "    init = 'uniform'\n",
    "    optimizer = 'adam'\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    NN_model.add(Dense(128, \n",
    "                   kernel_initializer='normal',\n",
    "                   input_dim = X_train.shape[1], \n",
    "                   activation=activation))\n",
    "\n",
    "    # The Hidden Layers :\n",
    "    NN_model.add(Dense(256, \n",
    "                       kernel_initializer='normal',\n",
    "                       activation=activation))\n",
    "    NN_model.add(Dense(256, \n",
    "                       kernel_initializer='normal',\n",
    "                       activation=activation))\n",
    "    NN_model.add(Dropout(dropout_rate)) \n",
    "    NN_model.add(Dense(256, \n",
    "                       kernel_initializer='normal',\n",
    "                       activation=activation))\n",
    "\n",
    "    # The Output Layer :\n",
    "    NN_model.add(Dense(1, kernel_initializer='normal',\n",
    "                       activation='linear'))\n",
    "\n",
    "    \n",
    "    NN_model.compile(loss='mean_squared_error', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['mean_absolute_error',\n",
    "                           'mean_squared_error'])\n",
    "    return NN_model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                        epochs = 10, \n",
    "                        batch_size = 10,\n",
    "                        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search \n",
    "activation =  ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'] # softmax, softplus, softsign \n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "weight_constraint=[1, 2, 3, 4, 5]\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "optimizer = [ 'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "##############################################################\n",
    "# grid search epochs, batch size\n",
    "epochs = [1, 10, 50, 100] # add 50, 100, 150 etc\n",
    "batch_size = [10, 32, 64, 100] # add 5, 10, 20, 40, 60, 80, 100 etc\n",
    "param_grid = dict(epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "#                   activation=activation,\n",
    "#                  momentum=momentum,\n",
    "#                  learn_rate=learn_rate,\n",
    "#                  dropout_rate=dropout_rate,\n",
    "#                  weight_constraint=weight_constraint,\n",
    "#                  neurons=neurons,\n",
    "#                  init=init,\n",
    "#                  optimizer=optimizer\n",
    "                 )\n",
    "##############################################################\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1, \n",
    "                    scoring=['r2'],\n",
    "                   refit = 'r2')\n",
    "grid_result = grid.fit(X_train, Y_train) \n",
    "##############################################################\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.fit(X_train, \n",
    "             Y_train, \n",
    "             epochs=500, \n",
    "             batch_size=32, \n",
    "             validation_split = 0.2,\n",
    "             callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'Weights-957--230.78113.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_pred = list(NN_model.predict(X_test)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_forecast = ({'Actual': Y_test, 'Predicted': neural_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, neural_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, neural_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, neural_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
    "lasso001.fit(X_train,Y_train)\n",
    "train_score001=lasso001.score(X_train,Y_train)\n",
    "test_score001=lasso001.score(X_test,Y_test)\n",
    "coeff_used001 = np.sum(lasso001.coef_!=0)\n",
    "print (\"training score for alpha=0.01:\", train_score001 )\n",
    "print (\"test score for alpha =0.01: \", test_score001)\n",
    "print (\"number of features used: for alpha =0.01:\", coeff_used001)\n",
    "\n",
    "lasso_pred = lasso001.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, lasso_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, lasso_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, lasso_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
